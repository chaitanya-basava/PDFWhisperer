{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "id": "2f99c96767860118",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import fitz\n",
    "from io import BytesIO\n",
    "from langchain import FAISS\n",
    "from typing import Optional\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from model import LLM, LLMBuilder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ],
   "id": "fbc66f33a2d49ef1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_text_and_urls(\n",
    "        uploaded_pdf: Optional[BytesIO] = None,\n",
    "        pdf_path: Optional[str] = None\n",
    ") -> str:\n",
    "    doc = (\n",
    "        fitz.open(pdf_path) if pdf_path\n",
    "        else fitz.open(stream=uploaded_pdf.read(), filetype=\"pdf\")\n",
    "    )\n",
    "    _text = \"\"\n",
    "\n",
    "    for page in doc:\n",
    "        # Extract text and append to the _text variable\n",
    "        _text += page.get_text()\n",
    "\n",
    "        # Extract links and the corresponding text\n",
    "        links = page.get_links()\n",
    "        for link in links:\n",
    "            if \"uri\" in link and \"from\" in link:\n",
    "                link_uri = link[\"uri\"]\n",
    "                link_rect = fitz.Rect(link[\"from\"])\n",
    "\n",
    "                link_text = page.get_text(\"text\", clip=link_rect)\n",
    "                # Append the link URI to the extracted text\n",
    "                _text += f\"\\n{link_text} ({link_uri})\"\n",
    "    \n",
    "    doc.close()\n",
    "    return _text"
   ],
   "id": "f738856e52342b70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_pdf(pdf_path: str) -> FAISS:\n",
    "    embeddings = CohereEmbeddings(model=\"embed-multilingual-v2.0\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    \n",
    "    text = extract_text_and_urls(pdf_path=pdf_path)\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    return FAISS.from_texts(chunks, embeddings)"
   ],
   "id": "55489577eea970d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "knowledge_base = process_pdf(\"/Users/chaitanyabasava/Desktop/Sai Naga Viswa Chaitanya_Basava_resume.pdf\")\n",
    "retriever = knowledge_base.as_retriever()"
   ],
   "id": "17b24082fa33e73a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "llm = LLMBuilder.get_llm(LLM.get_llm_by_id(5))",
   "id": "4d722d1afcc1553d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")"
   ],
   "id": "688a3cbc7ecd8664",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ],
   "id": "368ce4fb09233198",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ],
   "id": "e1cc8f14e64b70be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    return SQLChatMessageHistory(session_id, \"sqlite:///memory.db\")\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ],
   "id": "9adffa1aeb374da2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    }\n",
    ")"
   ],
   "id": "656d55a3e8a67b5e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
